{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SystemVerilog coverage using Athena AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run this demo:\n",
    "\n",
    "- Create yourself an AWS account - easy and free\n",
    "- Add an IAM user with S3 full permissions and Athena full permissions\n",
    "- Fill in the IAM user credentials below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env AWS_ACCESS_KEY_ID add_your_key_id_here\n",
    "%set_env AWS_SECRET_ACCESS_KEY add_your_secret_access_key_here\n",
    "%set_env AWS_DEFAULT_REGION us-east-1\n",
    "\n",
    "# let's test that your credentials are working correct\n",
    "! aws s3api list-buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The raw data:\n",
    "- transactions_log - is the log of the transactions on an interface\n",
    "- type_information - is the type of each field printed in the transaction_log\n",
    "\n",
    "This information is generated by the following example: https://www.edaplayground.com/x/2hJC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_log = \\\n",
    "\"\"\"# Time:    0, dir: RD, addr: 1490070710, burst: WRAP, len: 15, id:  3, lock: EXCLUSIVE,\n",
    "# Time:   26, dir: WR, addr:  165377426, burst: INCR, len:  0, id: 12, lock: NORMAL,\n",
    "# Time:   61, dir: WR, addr: 2328599037, burst: INCR, len:  8, id: 13, lock: NORMAL,\n",
    "# Time:  110, dir: RD, addr: 2328599037, burst: WRAP, len:  7, id:  5, lock: EXCLUSIVE,\n",
    "# Time:  133, dir: RD, addr:  165377426, burst: WRAP, len:  3, id:  7, lock: NORMAL,\n",
    "# Time:  181, dir: WR, addr: 1490070710, burst: WRAP, len:  6, id: 11, lock: EXCLUSIVE,\n",
    "# Time:  207, dir: RD, addr: 1490070710, burst: FIXED, len: 15, id:  5, lock: EXCLUSIVE,\n",
    "# Time:  231, dir: RD, addr: 1490070710, burst: INCR, len:  6, id:  2, lock: NORMAL,\n",
    "# Time:  252, dir: RD, addr:  165377426, burst: WRAP, len:  9, id:  0, lock: NORMAL,\n",
    "# Time:  290, dir: WR, addr: 2328599037, burst: WRAP, len: 10, id: 11, lock: NORMAL,\n",
    "# Time:  300, dir: WR, addr: 1490070710, burst: INCR, len:  7, id:  4, lock: EXCLUSIVE,\n",
    "# Time:  313, dir: WR, addr: 3668650794, burst: WRAP, len: 12, id:  8, lock: NORMAL,\n",
    "# Time:  358, dir: RD, addr: 3668650794, burst: INCR, len: 12, id:  7, lock: NORMAL,\n",
    "# Time:  403, dir: WR, addr: 3668650794, burst: WRAP, len: 10, id: 11, lock: NORMAL,\n",
    "# Time:  445, dir: RD, addr: 2328599037, burst: INCR, len:  9, id:  2, lock: EXCLUSIVE,\n",
    "\"\"\"\n",
    "\n",
    "type_information = \\\n",
    "\"\"\"# Transaction meta: dir: enum{RD=32'sd0,WR=32'sd1}axi_vip::dir_t, addr:          32, burst: enum{FIXED=32'sd0,INCR=32'sd1,WRAP=32'sd2}axi_vip::burst_t, len:           4, id:           4, lock: enum{NORMAL=32'sd0,EXCLUSIVE=32'sd1,LOCKED=32'sd2}axi_vip::lock_t\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data and upload "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some general purpose packages\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import pandas\n",
    "\n",
    "# python package for AWS access\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create files to upload\n",
    "\n",
    "- transactions.log - contains the transactions as shown above\n",
    "- types.json - contains the values for each enumeration used\n",
    "- columns.csv - contains information about the type of each column\n",
    "\n",
    "All these files will be uploaded to S3, and then turned into Athena tables. Note that they all have different formats that Athena can digest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.getcwd()\n",
    "\n",
    "transactions_filename = script_dir + \"/simple_tb/test1/axi_master_1/log/transactions.log\"\n",
    "columns_filename = script_dir + \"/simple_tb/test1/axi_master_1/columns/columns.csv\"\n",
    "types_filename = script_dir + \"/simple_tb/types_info/types.json\"\n",
    "\n",
    "for path in [transactions_filename, columns_filename, types_filename]:\n",
    "    os.makedirs(os.path.dirname(path)) \n",
    "\n",
    "transactions_file = open(transactions_filename, \"w\")\n",
    "columns_file = open(columns_filename, \"w\")\n",
    "types_file = open(types_filename, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract type/column information\n",
    "\n",
    "Turn the \"type_information\" variable into:\n",
    "- A file that has all enum values nicely organized in rows, \n",
    "- Another file that maps column name (i.e. \"dir\") to type (i.e. \"dir_t\")\n",
    "- A variable that stores SQL column definitions (i.e. dir -> string, len -> smallint)\n",
    "\n",
    "Also write the transactions into a file without any processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_information = type_information.split(\"# Transaction meta: \",1)[1]\n",
    "columns_defs = \"\"\n",
    "\n",
    "for type in type_information.split(\", \"):\n",
    "    if re.match('.*enum', type):\n",
    "        match = re.match('([^:]*): enum\\{(.*)\\}([^,]*)', type)\n",
    "        if match:\n",
    "            column_name = match.group(1)\n",
    "            enum_type_name = match.group(3)\n",
    "            columns_defs = columns_defs + \"`\" + column_name + \"` string,\\n\" \n",
    "            columns_file.write(column_name + \",\" + enum_type_name + \",0\\n\")\n",
    "            for enum_key_value in match.group(2).split(','):\n",
    "                match = re.match('([^=]+)=.*([0-9]+)', enum_key_value)\n",
    "                if match:\n",
    "                    entry = {}\n",
    "                    entry['enum_type_name'] = enum_type_name\n",
    "                    entry['enum_string'] = match.group(1)\n",
    "                    entry['enum_int'] = match.group(2)\n",
    "                    types_file.write(json.dumps(entry) + \"\\n\")\n",
    "    else:\n",
    "        match = re.match('([^ ]+): *([0-9]+)$', type)\n",
    "        if match:\n",
    "            column_name = match.group(1)\n",
    "            width = match.group(2)\n",
    "            width_int = int(width)\n",
    "            columns_file.write(column_name + \",\\t\\tint,\\t\\t\" + width + \"\\n\")\n",
    "            if width_int <= 15:\n",
    "                columns_defs = columns_defs + \"`\" + column_name + \"` smallint,\\n\"\n",
    "            elif width_int <= 31:\n",
    "                columns_defs = columns_defs + \"`\" + column_name + \"` int,\\n\"\n",
    "            else:\n",
    "                columns_defs = columns_defs + \"`\" + column_name + \"` bigint,\\n\"\n",
    "\n",
    "        else:\n",
    "            print(\"error parsing meta data: int type: \" + type + \" doesn't have a width field\")\n",
    "\n",
    "transactions_file.write(transaction_log)\n",
    "\n",
    "transactions_file.close()\n",
    "columns_file.close()\n",
    "types_file.close()\n",
    "\n",
    "columns_defs = columns_defs[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3 mb s3://coverage-demo/\n",
    "aws s3 sync simple_tb/ s3://coverage-demo/simple_tb --delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create database structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a blocking query function\n",
    "\n",
    "A thin wrapper around boto3 to run a query and wait for it to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, database, s3_output):\n",
    "    client = boto3.client('athena', \n",
    "                          aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                          aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "    \n",
    "    if database:\n",
    "        response = client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            QueryExecutionContext={\n",
    "                'Database': database\n",
    "            },\n",
    "            ResultConfiguration={\n",
    "                'OutputLocation': s3_output,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        response = client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            ResultConfiguration={\n",
    "                'OutputLocation': s3_output,\n",
    "            }\n",
    "        )\n",
    "    print('Execution ID: ' + response['QueryExecutionId'])\n",
    "\n",
    "    # wait for query to finish\n",
    "    while True:\n",
    "        execution = client.get_query_execution(QueryExecutionId=response['QueryExecutionId'])\n",
    "        if execution['QueryExecution']['Status']['State'] in ['SUCCEEDED', 'FAILED']:\n",
    "            print(execution['QueryExecution']['Status']['State'])\n",
    "            break\n",
    "        else:\n",
    "            print(execution['QueryExecution']['Status']['State'])\n",
    "            time.sleep(2) \n",
    "\n",
    "    response = client.get_query_results(QueryExecutionId=response['QueryExecutionId'])\n",
    "    if response['ResultSet']['Rows']:\n",
    "        row_num = 0;\n",
    "        for raw_row in response['ResultSet']['Rows']:\n",
    "            row = list()\n",
    "            for cell in raw_row['Data']:\n",
    "                row.append(list(cell.values())[0])\n",
    "            if row_num is 0:\n",
    "                df = pandas.DataFrame(columns=row)\n",
    "            else:\n",
    "                df.loc[row_num] = row\n",
    "            row_num += 1\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tables\n",
    "\n",
    "first clean the database, then recreate it all the tables in it. Note that they're created from the files uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_base = 's3://coverage-demo/'\n",
    "s3_output = 's3://coverage-demo/results/'\n",
    "database = 'coverage_demo'\n",
    "        \n",
    "drop_db = \"DROP DATABASE IF EXISTS %s CASCADE;\" % (database)\n",
    "\n",
    "create_db = \"CREATE DATABASE %s;\" % (database)\n",
    "\n",
    "create_enum_tbl = \\\n",
    "    \"\"\"CREATE EXTERNAL TABLE %s.%s (\n",
    "    `enum_type_name` string,\n",
    "    `enum_string` string,\n",
    "    `enum_int` int\n",
    "     )\n",
    "     ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "     WITH SERDEPROPERTIES (\n",
    "         'serialization.format' = '1'\n",
    "     ) LOCATION '%s'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false');\"\"\" % ( database, \"enums_info\", s3_base + \"/simple_tb/types_info/\" )\n",
    "\n",
    "print(create_enum_tbl + \"\\n\")\n",
    "\n",
    "create_columns_tbl = \\\n",
    "    \"\"\"CREATE EXTERNAL TABLE %s.%s (\n",
    "    `column_name` string,\n",
    "    `column_type` string,\n",
    "    `column_width` int\n",
    "     )\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "     WITH SERDEPROPERTIES (\n",
    "         'separatorChar' = ','\n",
    "     ) LOCATION '%s'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false');\"\"\" % ( database, \"columns_info\", s3_base + \"simple_tb/test1/axi_master_1/columns/\" )\n",
    "\n",
    "print(create_columns_tbl + \"\\n\")\n",
    "\n",
    "create_tr_tbl = \\\n",
    "    \"\"\"CREATE EXTERNAL TABLE %s.%s (\n",
    "    `time` bigint,\n",
    "    %s\n",
    "     )\n",
    "     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "     WITH SERDEPROPERTIES (  \n",
    "         'input.regex'='# Time: *([^ ^,]*), dir: *([^ ^,]*), addr: *([^ ^,]*), burst: *([^ ^,]*), len: *([^ ^,]*), id: *([^ ^,]*), lock: *([^ ^,]*),' \n",
    "     ) LOCATION '%s'\n",
    "     TBLPROPERTIES ('has_encrypted_data'='false');\"\"\" % ( database, \"axi_if1_transactions\", columns_defs, s3_base + \"simple_tb/test1/axi_master_1/log/\" )\n",
    "\n",
    "print(create_tr_tbl + \"\\n\")\n",
    "\n",
    "for query in [drop_db, create_db]:\n",
    "    run_query(query, \"\", s3_output)\n",
    "\n",
    "for query in [create_enum_tbl, create_columns_tbl, create_tr_tbl]:\n",
    "    run_query(query, database, s3_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interrupted RMW query\n",
    "\n",
    "Looking for exclusive read-exclusive write pair with a write in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interrupted_rmw = \"\"\"select first_tr.addr as addr, first_tr.time as read_time,  min(middle_tr.time) as interrupted_at, min(second_tr.time) as write_time\n",
    "from (\n",
    "    select row_number() over () as num, inner1.time, inner1.addr, inner1.dir, inner1.lock from\n",
    "        axi_if1_transactions inner1\n",
    "    where\n",
    "        inner1.dir = 'WR' or\n",
    "        inner1.lock = 'EXCLUSIVE'\n",
    "    order by inner1.addr, inner1.time\n",
    "    ) first_tr,\n",
    "    (\n",
    "    select row_number() over () as num, inner1.time, inner1.addr, inner1.dir, inner1.lock from\n",
    "        axi_if1_transactions inner1\n",
    "    where\n",
    "        inner1.dir = 'WR' or\n",
    "        inner1.lock = 'EXCLUSIVE'\n",
    "    order by inner1.addr, inner1.time\n",
    "    ) second_tr,\n",
    "    (\n",
    "    select row_number() over () as num, inner1.time, inner1.addr, inner1.dir, inner1.lock from\n",
    "        axi_if1_transactions inner1\n",
    "    where\n",
    "        inner1.dir = 'WR' or\n",
    "        inner1.lock = 'EXCLUSIVE'\n",
    "    order by inner1.addr, inner1.time\n",
    "    ) middle_tr\n",
    "where first_tr.addr = second_tr.addr and\n",
    "         second_tr.addr = first_tr.addr and\n",
    "     first_tr.lock = 'EXCLUSIVE' and\n",
    "     second_tr.lock = 'EXCLUSIVE' and\n",
    "     first_tr.dir = 'RD' and\n",
    "     second_tr.dir = 'WR' and\n",
    "     middle_tr.dir = 'WR' and\n",
    "     first_tr.num < middle_tr.num and\n",
    "     middle_tr.num < second_tr.num\n",
    "group by 1,2;\"\"\"\n",
    "\n",
    "for query in [interrupted_rmw]:\n",
    "    df = run_query(query, database, s3_output)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burst cross RD/WR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burst_x_dir = \"\"\"select distinct expected_values.burst, expected_values.dir, if(axi_if1_transactions.burst is not null, 'TRUE', 'FALSE') as covered from axi_if1_transactions right outer join\n",
    "((\n",
    "  select enums_info.enum_string as burst from enums_info,\n",
    "  (\n",
    "    select columns_info.column_type from columns_info \n",
    "    where column_name = 'burst'\n",
    "    ) column_meta \n",
    "  where column_meta.column_type = enums_info.enum_type_name\n",
    "  ) enum1_values\n",
    "cross join (\n",
    "  select enums_info.enum_string as dir from enums_info,\n",
    "  (\n",
    "    select columns_info.column_type from columns_info \n",
    "    where column_name = 'dir'\n",
    "    ) column_meta \n",
    "  where column_meta.column_type = enums_info.enum_type_name\n",
    "  ) enum2_values\n",
    ") expected_values \n",
    "on expected_values.burst = axi_if1_transactions.burst and\n",
    "   expected_values.dir = axi_if1_transactions.dir\n",
    "order by expected_values.burst, expected_values.dir\"\"\"\n",
    "    \n",
    "for query in [burst_x_dir]:\n",
    "    df = run_query(query, database, s3_output)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
